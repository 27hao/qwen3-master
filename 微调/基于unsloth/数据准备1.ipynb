{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCTJshvYfnGl"
      },
      "source": [
        "## 下载环境"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhxGsReYfnGm"
      },
      "source": [
        "pip install transformers\n",
        "pip install modelscope\n",
        "pip install datasets\n",
        "pip install accelerate\n",
        "pip install bitsandbytes\n",
        "pip install peft\n",
        "pip install swanlab\n",
        "pip install sentencepiece\n",
        "pip install trl\n",
        "\n",
        "\n",
        "pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth modelscope datasets accelerate bitsandbytes peft swanlab sentencepiece trl unsloth_zero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmxTV52ZmoHo",
        "outputId": "aa66c84d-fc32-4eb2-f6b0-452f95916f6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.11.2)\n",
            "Requirement already satisfied: modelscope in /usr/local/lib/python3.12/dist-packages (1.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: swanlab in /usr/local/lib/python3.12/dist-packages (0.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth_zero (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth_zero\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset,Dataset\n",
        "from swanlab.integration.transformers import SwanLabCallback\n",
        "import pandas as pd\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TextStreamer\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "from typing import Optional, List, Union\n",
        "import sys\n",
        "import deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "CNTFI7WSkCEb",
        "outputId": "e33b626a-1fb8-487e-cb38-ac88cc48b833"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3680519819.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 将转换后的推理数据集应用对话模板\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m reasoning_conversations = tokenizer.apply_chat_template(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mds_reason_alpaca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"conversations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    # load_in_4bit = True,\n",
        "    # load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        "    # device_map=device_map\n",
        "    # token = \"\",\n",
        ")"
      ],
      "metadata": {
        "id": "A6Fjj4JSlWuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,           #  LoRA秩，建议值为8,16,32,64,128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # LoRA alpha值，建议设为rank或rank*2\n",
        "    lora_dropout = 0.1, # LoRA dropout，0值经过优化\n",
        "    bias = \"none\",    # 偏置设置，\"none\"已优化\n",
        "\n",
        "    # [新特性] \"unsloth\"模式减少30%显存，可适应2倍大的批次大小\n",
        "    use_gradient_checkpointing = \"unsloth\", #梯度检查点，用于长上下文\n",
        "    random_state = 3407,  # 随机种子\n",
        "    use_rslora = False,   # 是否使用rank stabilized LoRA\n",
        "    loftq_config = None,  # LoftQ配置\n",
        ")"
      ],
      "metadata": {
        "id": "RvrdIWXbldBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "jsonl_data_file = \"afsim-data.jsonl\" # 您的数据文件\n",
        "ds_reason = load_dataset(\"json\", data_files={\"train\": jsonl_data_file}, split=\"train\")\n",
        "ds_reason[0]\n",
        "#ds_no_reason = load_dataset(\"BAAI/IndustryInstruction_Health-Medicine\",cache_dir = './data/no_reason')"
      ],
      "metadata": {
        "id": "bNJDbq8WliNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_alpaca_format(examples):\n",
        "    conversations = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        # 提取系统提示词\n",
        "        system_prompt = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"system\"), \"\")\n",
        "\n",
        "        # 提取用户问题和助手回答\n",
        "        user_question = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
        "        assistant_answer = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"assistant\"), \"\")\n",
        "\n",
        "        # 分割think和response部分\n",
        "        think_part = \"\"\n",
        "        response_part = \"\"\n",
        "        if assistant_answer:\n",
        "            think_start = assistant_answer.find(\"<think>\")\n",
        "            think_end = assistant_answer.find(\"</think>\")\n",
        "            if think_start != -1 and think_end != -1:\n",
        "                think_part = assistant_answer[think_start+7:think_end].strip()\n",
        "\n",
        "            response_start = assistant_answer.find(\"<response>\")\n",
        "            response_end = assistant_answer.find(\"</response>\")\n",
        "            if response_start != -1 and response_end != -1:\n",
        "                response_part = assistant_answer[response_start+10:response_end].strip()\n",
        "            else:\n",
        "                # 如果没有response标签，尝试获取think标签之后的内容\n",
        "                response_part = assistant_answer[think_end+8:].strip()\n",
        "\n",
        "        # 构建Alpaca格式的对话\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_question},\n",
        "            {\"role\": \"assistant\", \"content\": f'<think>{think_part}</think>{response_part}'}\n",
        "        ]\n",
        "        conversations.append(conversation)\n",
        "\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "# 批量处理数据集\n",
        "ds_reason_alpaca = ds_reason.map(\n",
        "    convert_to_alpaca_format,\n",
        "    batched=True,\n",
        "    remove_columns=ds_reason.column_names,\n",
        ")\n",
        "\n",
        "# 将转换后的推理数据集应用对话模板\n",
        "reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    ds_reason_alpaca[\"conversations\"],\n",
        "    tokenize = False\n",
        ")\n",
        "\n",
        "# 合并两个数据集\n",
        "data = pd.concat([\n",
        "    pd.Series(reasoning_conversations)    # 推理对话数据\n",
        "])\n",
        "\n",
        "data.name = \"text\"  # 设置数据列名为\"text\"\n",
        "\n",
        "# 将合并的数据转换为HuggingFace Dataset格式\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "# 随机打乱数据集\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)"
      ],
      "metadata": {
        "id": "rw8ZxEhClmip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swanlab_callback = SwanLabCallback(\n",
        "    project=\"Qwen3-8B-fintune\",\n",
        "    experiment_name=\"Qwen3-8B-combind\",\n",
        "    description=\"使用通义千问Qwen3-8B模型在FreedomIntelligence/medical-o1-reasoning-SFT和BAAI/IndustryInstruction_Health-Medicine数据集上微调。\",\n",
        "    config={\n",
        "        \"model\": \"Qwen/Qwen3-8B\",\n",
        "        \"dataset\": \"https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "        \"train_data_number\": len(combined_dataset),\n",
        "        \"lora_rank\": 8,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"lora_dropout\": 0.1,\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field = \"text\",\n",
        "    train_dataset = combined_dataset,\n",
        "    eval_dataset = None,  # 可以设置评估数据集\n",
        "    callbacks=[swanlab_callback],\n",
        "    args = SFTConfig(\n",
        "        output_dir=\"./lora_model\",\n",
        "        per_device_train_batch_size = 1,  # 每个设备的训练批次大小\n",
        "        gradient_accumulation_steps = 4,  # 使用梯度累积模拟更大批次大小\n",
        "        warmup_steps = 5,  # 预热步数\n",
        "        num_train_epochs = 7,\n",
        "        learning_rate = 2e-4,   # 学习率（长期训练可降至2e-5）\n",
        "        logging_steps = 5,  # 日志记录间隔\n",
        "        optim = \"adamw_8bit\",  # 优化器\n",
        "        weight_decay = 0.01,  # 权重衰减\n",
        "        lr_scheduler_type = \"linear\",  # 学习率调度类型\n",
        "        seed = 3407,  # 随机种子\n",
        "        report_to = \"none\",   # 可设置为\"wandb\"等进行实验追踪\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "        max_grad_norm=1.0,\n",
        "        # deepspeed=DS_CONFIG,\n",
        "        logging_first_step=5,\n",
        "        save_steps=100,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "d9M_DgUsl5j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 显示当前内存统计\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "R17IH_YKl78g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "GjBb8KJjl-Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 显示最终内存和时间统计\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "dKfoAiBEl_tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "c-eYjXkumBL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n95PtK6IfnGn"
      },
      "source": [
        "## 注意事项"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将转换后的推理数据集应用对话模板\n",
        "reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    ds_reason_alpaca[\"conversations\"],\n",
        "    tokenize = False\n",
        ")"
      ],
      "metadata": {
        "id": "a2ktdfIzj-36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "J4Wv7rLifnGn"
      },
      "source": [
        "运行代码前先开启学术加速/梯子\n",
        "\n",
        "只能单卡启动，因为是free模式\n",
        "\n",
        "强行用device_map多卡启动也没用，训练的时候会报错\n",
        "\n",
        "sfttrainer中的 processing_class=tokenizer 改成 tokenizer=tokenizer 因为这里是unsloth包装过的sfttrainer\n",
        "\n",
        "用 python train.py运行，不要用deepspeed运行 （不要用deepspeed --include 'localhost:0' train.py 启动）"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}