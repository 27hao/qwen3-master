{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCTJshvYfnGl"
      },
      "source": [
        "## ä¸‹è½½ç¯å¢ƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhxGsReYfnGm"
      },
      "source": [
        "pip install transformers\n",
        "pip install modelscope\n",
        "pip install datasets\n",
        "pip install accelerate\n",
        "pip install bitsandbytes\n",
        "pip install peft\n",
        "pip install swanlab\n",
        "pip install sentencepiece\n",
        "pip install trl\n",
        "\n",
        "\n",
        "pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth modelscope datasets accelerate bitsandbytes peft swanlab sentencepiece trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmxTV52ZmoHo",
        "outputId": "aa66c84d-fc32-4eb2-f6b0-452f95916f6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.12/dist-packages (2025.11.2)\n",
            "Requirement already satisfied: modelscope in /usr/local/lib/python3.12/dist-packages (1.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: swanlab in /usr/local/lib/python3.12/dist-packages (0.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.23.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth_zero (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth_zero\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset,Dataset\n",
        "from swanlab.integration.transformers import SwanLabCallback\n",
        "import pandas as pd\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import TextStreamer\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "from typing import Optional, List, Union\n",
        "import sys\n",
        "import deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNTFI7WSkCEb",
        "outputId": "8cbddbec-3d9d-414f-ed5d-20fa27e363eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        "    # device_map=device_map\n",
        "    # token = \"\",\n",
        ")"
      ],
      "metadata": {
        "id": "A6Fjj4JSlWuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,           #  LoRAç§©ï¼Œå»ºè®®å€¼ä¸º8,16,32,64,128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # LoRA alphaå€¼ï¼Œå»ºè®®è®¾ä¸ºrankæˆ–rank*2\n",
        "    lora_dropout = 0.1, # LoRA dropoutï¼Œ0å€¼ç»è¿‡ä¼˜åŒ–\n",
        "    bias = \"none\",    # åç½®è®¾ç½®ï¼Œ\"none\"å·²ä¼˜åŒ–\n",
        "\n",
        "    # [æ–°ç‰¹æ€§] \"unsloth\"æ¨¡å¼å‡å°‘30%æ˜¾å­˜ï¼Œå¯é€‚åº”2å€å¤§çš„æ‰¹æ¬¡å¤§å°\n",
        "    use_gradient_checkpointing = \"unsloth\", #æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œç”¨äºé•¿ä¸Šä¸‹æ–‡\n",
        "    random_state = 3407,  # éšæœºç§å­\n",
        "    use_rslora = False,   # æ˜¯å¦ä½¿ç”¨rank stabilized LoRA\n",
        "    loftq_config = None,  # LoftQé…ç½®\n",
        ")"
      ],
      "metadata": {
        "id": "RvrdIWXbldBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "jsonl_data_file = \"afsim-data.jsonl\" # æ‚¨çš„æ•°æ®æ–‡ä»¶\n",
        "ds_reason = load_dataset(\"json\", data_files={\"train\": jsonl_data_file}, split=\"train\")\n",
        "ds_reason[0]\n",
        "#ds_no_reason = load_dataset(\"BAAI/IndustryInstruction_Health-Medicine\",cache_dir = './data/no_reason')"
      ],
      "metadata": {
        "id": "bNJDbq8WliNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_alpaca_format(examples):\n",
        "    conversations = []\n",
        "    for messages in examples[\"messages\"]:\n",
        "        # æå–ç³»ç»Ÿæç¤ºè¯\n",
        "        system_prompt = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"system\"), \"\")\n",
        "\n",
        "        # æå–ç”¨æˆ·é—®é¢˜å’ŒåŠ©æ‰‹å›ç­”\n",
        "        user_question = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
        "        assistant_answer = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"assistant\"), \"\")\n",
        "\n",
        "        # åˆ†å‰²thinkå’Œresponseéƒ¨åˆ†\n",
        "        think_part = \"\"\n",
        "        response_part = \"\"\n",
        "        if assistant_answer:\n",
        "            think_start = assistant_answer.find(\"<think>\")\n",
        "            think_end = assistant_answer.find(\"</think>\")\n",
        "            if think_start != -1 and think_end != -1:\n",
        "                think_part = assistant_answer[think_start+7:think_end].strip()\n",
        "\n",
        "            response_start = assistant_answer.find(\"<response>\")\n",
        "            response_end = assistant_answer.find(\"</response>\")\n",
        "            if response_start != -1 and response_end != -1:\n",
        "                response_part = assistant_answer[response_start+10:response_end].strip()\n",
        "            else:\n",
        "                # å¦‚æœæ²¡æœ‰responseæ ‡ç­¾ï¼Œå°è¯•è·å–thinkæ ‡ç­¾ä¹‹åçš„å†…å®¹\n",
        "                response_part = assistant_answer[think_end+8:].strip()\n",
        "\n",
        "        # æ„å»ºAlpacaæ ¼å¼çš„å¯¹è¯\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_question},\n",
        "            {\"role\": \"assistant\", \"content\": f'<think>{think_part}</think>{response_part}'}\n",
        "        ]\n",
        "        conversations.append(conversation)\n",
        "\n",
        "    return {\"conversations\": conversations}\n",
        "\n",
        "# æ‰¹é‡å¤„ç†æ•°æ®é›†\n",
        "ds_reason_alpaca = ds_reason.map(\n",
        "    convert_to_alpaca_format,\n",
        "    batched=True,\n",
        "    remove_columns=ds_reason.column_names,\n",
        ")\n",
        "\n",
        "# å°†è½¬æ¢åçš„æ¨ç†æ•°æ®é›†åº”ç”¨å¯¹è¯æ¨¡æ¿\n",
        "reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    ds_reason_alpaca[\"conversations\"],\n",
        "    tokenize = False\n",
        ")\n",
        "\n",
        "# åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†\n",
        "data = pd.concat([\n",
        "    pd.Series(reasoning_conversations)    # æ¨ç†å¯¹è¯æ•°æ®\n",
        "])\n",
        "\n",
        "data.name = \"text\"  # è®¾ç½®æ•°æ®åˆ—åä¸º\"text\"\n",
        "\n",
        "# å°†åˆå¹¶çš„æ•°æ®è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "# éšæœºæ‰“ä¹±æ•°æ®é›†\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)"
      ],
      "metadata": {
        "id": "rw8ZxEhClmip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swanlab_callback = SwanLabCallback(\n",
        "    project=\"Qwen3-8B-fintune\",\n",
        "    experiment_name=\"Qwen3-8B-combind\",\n",
        "    description=\"ä½¿ç”¨é€šä¹‰åƒé—®Qwen3-8Bæ¨¡å‹åœ¨FreedomIntelligence/medical-o1-reasoning-SFTå’ŒBAAI/IndustryInstruction_Health-Medicineæ•°æ®é›†ä¸Šå¾®è°ƒã€‚\",\n",
        "    config={\n",
        "        \"model\": \"Qwen/Qwen3-8B\",\n",
        "        \"dataset\": \"https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT\",\n",
        "        \"train_data_number\": len(combined_dataset),\n",
        "        \"lora_rank\": 8,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"lora_dropout\": 0.1,\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field = \"text\",\n",
        "    train_dataset = combined_dataset,\n",
        "    eval_dataset = None,  # å¯ä»¥è®¾ç½®è¯„ä¼°æ•°æ®é›†\n",
        "    callbacks=[swanlab_callback],\n",
        "    args = SFTConfig(\n",
        "        output_dir=\"./lora_model\",\n",
        "        per_device_train_batch_size = 1,  # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
        "        gradient_accumulation_steps = 4,  # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡å¤§å°\n",
        "        warmup_steps = 5,  # é¢„çƒ­æ­¥æ•°\n",
        "        num_train_epochs = 7,\n",
        "        learning_rate = 2e-4,   # å­¦ä¹ ç‡ï¼ˆé•¿æœŸè®­ç»ƒå¯é™è‡³2e-5ï¼‰\n",
        "        logging_steps = 5,  # æ—¥å¿—è®°å½•é—´éš”\n",
        "        optim = \"adamw_8bit\",  # ä¼˜åŒ–å™¨\n",
        "        weight_decay = 0.01,  # æƒé‡è¡°å‡\n",
        "        lr_scheduler_type = \"linear\",  # å­¦ä¹ ç‡è°ƒåº¦ç±»å‹\n",
        "        seed = 3407,  # éšæœºç§å­\n",
        "        report_to = \"none\",   # å¯è®¾ç½®ä¸º\"wandb\"ç­‰è¿›è¡Œå®éªŒè¿½è¸ª\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "        max_grad_norm=1.0,\n",
        "        # deepspeed=DS_CONFIG,\n",
        "        logging_first_step=5,\n",
        "        save_steps=100,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "d9M_DgUsl5j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ˜¾ç¤ºå½“å‰å†…å­˜ç»Ÿè®¡\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "R17IH_YKl78g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "GjBb8KJjl-Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ˜¾ç¤ºæœ€ç»ˆå†…å­˜å’Œæ—¶é—´ç»Ÿè®¡\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "dKfoAiBEl_tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "c-eYjXkumBL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n95PtK6IfnGn"
      },
      "source": [
        "## æ³¨æ„äº‹é¡¹"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å°†è½¬æ¢åçš„æ¨ç†æ•°æ®é›†åº”ç”¨å¯¹è¯æ¨¡æ¿\n",
        "reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    ds_reason_alpaca[\"conversations\"],\n",
        "    tokenize = False\n",
        ")"
      ],
      "metadata": {
        "id": "a2ktdfIzj-36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "J4Wv7rLifnGn"
      },
      "source": [
        "è¿è¡Œä»£ç å‰å…ˆå¼€å¯å­¦æœ¯åŠ é€Ÿ/æ¢¯å­\n",
        "\n",
        "åªèƒ½å•å¡å¯åŠ¨ï¼Œå› ä¸ºæ˜¯freeæ¨¡å¼\n",
        "\n",
        "å¼ºè¡Œç”¨device_mapå¤šå¡å¯åŠ¨ä¹Ÿæ²¡ç”¨ï¼Œè®­ç»ƒçš„æ—¶å€™ä¼šæŠ¥é”™\n",
        "\n",
        "sfttrainerä¸­çš„ processing_class=tokenizer æ”¹æˆ tokenizer=tokenizer å› ä¸ºè¿™é‡Œæ˜¯unslothåŒ…è£…è¿‡çš„sfttrainer\n",
        "\n",
        "ç”¨ python train.pyè¿è¡Œï¼Œä¸è¦ç”¨deepspeedè¿è¡Œ ï¼ˆä¸è¦ç”¨deepspeed --include 'localhost:0' train.py å¯åŠ¨ï¼‰"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}